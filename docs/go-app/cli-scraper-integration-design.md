# CLI Scraper Integration Design Document

## Overview

This document details the integration of the scraper service into the Go CLI application, enabling automatic content extraction when adding links. The flow will: (1) send URLs to the scraper service, (2) process the scraper response in the CLI, and (3) post the enriched link data to the API.

**Goal**: Automatically extract title and text content from URLs using the scraper service before saving links to the API.

**Timeline**: 1-2 days
**Complexity**: Medium
**Status**: ğŸŸ¡ **In Progress** - Configuration complete, form integration pending

**Prerequisites**:

- âœ… Scraper HTTP service implemented (`scraper/src/server.ts`)
- âœ… Go scraper client implemented (`link-mgmt-go/pkg/scraper/client.go`)
- âœ… CLI add link form implemented (`link-mgmt-go/pkg/cli/app.go`)
- âœ… API client implemented (`link-mgmt-go/pkg/cli/client/`)
- âœ… Nginx reverse proxy configured (`nginx/nginx.conf`, `docker-compose.yml`)
- âœ… Configuration updated with `BaseURL` and `ScrapeTimeout` (`pkg/config/config.go`)

---

## Architecture Overview

### Current Flow (Manual Entry)

```
User Input â†’ CLI Form â†’ API Client â†’ API Server â†’ Database
   â†“           â†“
URL, Title,   LinkCreate
Description,  JSON
Text
```

### Desired Flow (With Scraping)

```
User Input â†’ CLI Form â†’ Nginx â†’ Scraper Service â†’ CLI Processing â†’ Nginx â†’ API Client â†’ API Server â†’ Database
   â†“           â†“         â†“            â†“                  â†“            â†“          â†“
URL only   Scrape    /scrape      ScrapeResponse   Enrich Link   /api/v1   LinkCreate
           Request   endpoint                                    /links     JSON
```

---

## Detailed Flow

### Step-by-Step Process

1. **User Enters URL**
   - User provides URL in the add link form
   - CLI validates URL format
   - URL is stored temporarily in form state

2. **Scrape Request** (Automatic)
   - CLI checks if scraper service is configured
   - If configured, automatically sends scrape request to scraper service
   - Shows loading indicator to user ("Scraping URL...")
   - Waits for scraper response (with timeout handling)

3. **Scraper Response Handling**
   - CLI receives `ScrapeResponse` from scraper service
   - Processes response:
     - Maps `title` â†’ `LinkCreate.Title`
     - Maps `text` â†’ `LinkCreate.Text`
     - Preserves original `url`
     - Leaves `description` empty (can be manually added later)

4. **User Review/Edit** (Optional)
   - Pre-fills form fields with scraped data
   - User can review and edit title/text before submission
   - User can skip scraping if desired (flag or prompt)

5. **API Submission**
   - User submits form (or confirms pre-filled data)
   - CLI sends `LinkCreate` to API via HTTP client
   - API validates and saves to database
   - CLI displays success/error message

---

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚  Enters URL â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLI Add Link Form (Bubble Tea)     â”‚
â”‚  - URL Input Field                  â”‚
â”‚  - Title Input Field (auto-filled)  â”‚
â”‚  - Description Input Field          â”‚
â”‚  - Text Textarea (auto-filled)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [User enters URL]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper Service Client             â”‚
â”‚  (pkg/scraper/client.go)            â”‚
â”‚  - Scrape(url, timeout)             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [HTTP POST http://localhost/scrape]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nginx Reverse Proxy                â”‚
â”‚  (nginx/nginx.conf)                 â”‚
â”‚  - Routes /scrape â†’ scraper service â”‚
â”‚  - Routes /scraper/* â†’ scraper      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [HTTP POST http://scraper-dev:3000/scrape]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper Service                    â”‚
â”‚  (scraper/src/server.ts)            â”‚
â”‚  - BrowserManager.extractFromUrl()  â”‚
â”‚  - extractMainContent()             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [ScrapeResponse JSON]
       â”‚ { success, url, title, text }
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLI Response Processing            â”‚
â”‚  - Map title â†’ LinkCreate.Title     â”‚
â”‚  - Map text â†’ LinkCreate.Text       â”‚
â”‚  - Handle errors gracefully         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [Auto-fill form fields]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Reviews/Edits (Optional)      â”‚
â”‚  - Can modify title                 â”‚
â”‚  - Can modify text                  â”‚
â”‚  - Can add description              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [User confirms]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Client                         â”‚
â”‚  (pkg/cli/client/links.go)          â”‚
â”‚  - CreateLink(LinkCreate)           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [HTTP POST http://localhost/api/v1/links]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nginx Reverse Proxy                â”‚
â”‚  (nginx/nginx.conf)                 â”‚
â”‚  - Routes /api/* â†’ API service      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [HTTP POST http://api-dev:8080/api/v1/links]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server                         â”‚
â”‚  - Validates request                â”‚
â”‚  - Saves to database                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ [Link JSON response]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLI Success Display                â”‚
â”‚  - Shows created link details       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Configuration

### Current Config Structure

âœ… **Already Implemented**: The configuration has been updated to support nginx reverse proxy:

```go
// pkg/config/config.go
type Config struct {
    CLI struct {
        BaseURL       string `toml:"base_url"`       // Single base URL for all services (via nginx)
        APIKey        string `toml:"api_key"`
        ScrapeTimeout int    `toml:"scrape_timeout"` // Timeout for scraping operations in seconds
    } `toml:"cli"`
}
```

**Note**: The CLI constructs URLs as:

- API: `{base_url}/api/v1/*` â†’ nginx routes to `api-dev:8080`
- Scraper: `{base_url}/scrape` â†’ nginx routes to `scraper-dev:3000`
- Scraper Health: `{base_url}/scraper/health` â†’ nginx routes to `scraper-dev:3000/health`

### Default Values

âœ… **Already Implemented**: Default configuration:

```go
func DefaultConfig() *Config {
    cfg := &Config{}
    cfg.CLI.BaseURL = "http://localhost"  // nginx reverse proxy on port 80
    cfg.CLI.ScrapeTimeout = 30            // 30 seconds default
    return cfg
}
```

### Config File Example

```toml
[cli]
base_url = "http://localhost"
api_key = "your-api-key-here"
scrape_timeout = 30
```

### Nginx Routing Structure

The nginx reverse proxy (`nginx/nginx.conf`) routes requests as follows:

- **API Routes**:
    - `/api/*` â†’ `http://api-dev:8080/api/*` (or `http://api:8080` in production)
    - `/health` â†’ `http://api-dev:8080/health`

- **Scraper Routes**:
    - `/scrape` â†’ `http://scraper-dev:3000/scrape` (direct mapping)
    - `/scraper/*` â†’ `http://scraper-dev:3000/*` (prefix stripped via rewrite)
    - `/scraper/health` â†’ `http://scraper-dev:3000/health`

All services are accessed through nginx on port 80, which simplifies CLI configuration to a single `base_url`.

### Docker Compose Setup

The `docker-compose.yml` orchestrates all services with the following structure:

**Services (dev profile)**:

- **nginx**: Reverse proxy container (port 80 exposed to host)
- **api-dev**: Go API service (port 8080, internal only)
- **scraper-dev**: Bun scraper service (port 3000, internal only)
- **postgres**: PostgreSQL database (port 5432 exposed to host)

**Key Features**:

- All services communicate via Docker networking (service names as hostnames)
- Nginx depends on `api-dev` and `scraper-dev` services
- Health checks configured for all services
- Volume mounts for hot-reloading in development
- Services use `profiles: [dev]` for development environment

**CLI Access**:

- CLI runs on host machine (not in Docker)
- Accesses services via `http://localhost` (nginx on port 80)
- No need to know internal service ports or hostnames

---

## User Experience Flow

### Scenario 1: Successful Scraping

1. User runs: `./cli --add`
2. Form displays: "URL (required):"
3. User enters: `https://example.com/article`
4. User presses Enter
5. Form shows: "â³ Scraping URL... (this may take a few seconds)"
6. After 2-3 seconds:
   - Title field auto-fills: "Example Article Title"
   - Text field auto-fills: "Article content here..."
   - Cursor moves to Title field
   - Form shows: "âœ“ Scraped successfully"
7. User can:
   - Press Enter to skip Title (keep scraped value)
   - Edit Title if needed
   - Press Enter to continue
8. Form moves to Description (optional)
9. User presses Enter to skip
10. Form shows Text field (pre-filled)
11. User presses Enter to submit
12. Success: "âœ“ Link created successfully!"

### Scenario 2: Scraping Failure

1. User enters URL and presses Enter
2. Form shows: "â³ Scraping URL..."
3. After timeout or error:
   - Form shows: "âš ï¸  Could not scrape URL. Continue manually? (y/n)"
   - Title field remains empty
   - Text field remains empty
   - User can proceed to fill manually or retry

### Scenario 3: Scraper Service Unavailable

1. CLI checks scraper service health on startup (or first use)
2. If unavailable:
   - Logs warning: "Scraper service not available. Scraping disabled."
   - Form works normally but without auto-scraping
   - User manually enters all fields

### Scenario 4: User Skips Scraping

1. User enters URL
2. Before scraping starts, user presses a key (e.g., `s` for "skip")
3. Or user explicitly disables scraping via flag: `--add --no-scrape`
4. Form proceeds with manual entry only

---

## Implementation Details

### 1. Configuration âœ… COMPLETE

**File**: `link-mgmt-go/pkg/config/config.go`

**Status**: âœ… Already implemented

- âœ… `BaseURL` field exists in `CLI` struct (replaces `APIBaseURL`)
- âœ… `ScrapeTimeout` field exists in `CLI` struct
- âœ… `DefaultConfig()` sets defaults (`base_url = "http://localhost"`, `scrape_timeout = 30`)
- âœ… Config loading and saving implemented

### 2. Update CLI App Structure

**File**: `link-mgmt-go/pkg/cli/app.go`

**Changes**:

- Add scraper service client to `App` struct
- Add method to initialize scraper service: `getScraperService()`
- Pass scraper service to add link form
- Initialize scraper service with base URL (same as API, via nginx)

```go
type App struct {
    cfg            *config.Config
    client         *client.Client
    scraperService *scraper.ScraperService  // NEW
}

func (a *App) getScraperService() *scraper.ScraperService {
    if a.scraperService == nil {
        // Use same base URL as API (nginx routes /scraper/* to scraper service)
        baseURL := a.cfg.CLI.BaseURL
        a.scraperService = scraper.NewScraperService(baseURL)
    }
    return a.scraperService
}
```

**Note**: âœ… The scraper client (`pkg/scraper/client.go`) already uses the `/scrape` endpoint, which nginx routes correctly to the scraper service. Health checks use `/scraper/health` with fallback to `/health`.

### 3. Enhance Add Link Form

**File**: `link-mgmt-go/pkg/cli/app.go` (update `addLinkForm`)

**Changes**:

- Add scraper service to form struct
- Add scraping state: `scraping`, `scraped`, `scrapeError`
- Add scraping step after URL entry
- Auto-fill title and text from scrape response
- Add loading indicator UI

```go
type addLinkForm struct {
    client         *client.Client
    scraperService *scraper.ScraperService  // NEW
    urlInput       textinput.Model
    titleInput     textinput.Model
    descInput      textinput.Model
    textInput      textarea.Model
    step           int  // 0=URL, 1=Scraping, 2=Title, 3=Description, 4=Text, 5=Done
    scraping       bool // NEW: indicates scraping in progress
    scraped        bool // NEW: indicates scraping completed
    scrapeError    error // NEW: stores scraping error if any
    scrapedData    *scraper.ScrapeResponse // NEW: stores scrape result
    err            error
    created        *models.Link
}
```

### 4. Scraping Logic

**New Method in `addLinkForm`**:

```go
func (m *addLinkForm) scrapeURL(url string) tea.Cmd {
    return func() tea.Msg {
        if m.scraperService == nil {
            return scrapeErrorMsg{err: fmt.Errorf("scraper service not configured")}
        }

        // Check health first (optional, but good for UX)
        if err := m.scraperService.CheckHealth(); err != nil {
            return scrapeErrorMsg{err: fmt.Errorf("scraper service unavailable: %w", err)}
        }

        // Scrape the URL
        // Use timeout from config (already available via cfg.CLI.ScrapeTimeout)
        timeout := m.cfg.CLI.ScrapeTimeout
        result, err := m.scraperService.Scrape(url, timeout*1000) // timeout in ms
        if err != nil {
            return scrapeErrorMsg{err: err}
        }

        if !result.Success {
            return scrapeErrorMsg{err: fmt.Errorf("scraping failed: %s", result.Error)}
        }

        return scrapeSuccessMsg{result: result}
    }
}
```

**Note**: The scraper client (`pkg/scraper/client.go`) is already implemented and:

- âœ… Uses `/scrape` endpoint (routed via nginx)
- âœ… Has `CheckHealth()` method using `/scraper/health` endpoint
- âœ… Returns `ScrapeResponse` with `Success`, `Title`, `Text`, `URL`, `Error` fields

### 5. Message Types

**New message types for scraping state**:

```go
type scrapeStartMsg struct{}
type scrapeSuccessMsg struct {
    result *scraper.ScrapeResponse
}
type scrapeErrorMsg struct {
    err error
}
```

### 6. Form Update Logic

**Update `Update()` method**:

- Handle `scrapeStartMsg` â†’ set `scraping = true`, trigger scrape command
- Handle `scrapeSuccessMsg` â†’ set `scraping = false`, `scraped = true`, auto-fill fields, move to next step
- Handle `scrapeErrorMsg` â†’ set `scraping = false`, show error, allow manual entry

### 7. Form View Updates

**Update `View()` method**:

- Show "â³ Scraping URL..." when `scraping = true`
- Show "âœ“ Scraped successfully" when `scraped = true`
- Show error message when `scrapeError != nil`
- Pre-fill title and text fields with scraped data

---

## Error Handling Strategy

### Scraper Service Errors

1. **Service Unavailable** (connection refused, timeout):
   - Log warning
   - Allow manual entry
   - Don't block link creation

2. **Scraping Failed** (invalid URL, timeout, extraction error):
   - Show error message to user
   - Allow retry or manual entry
   - Don't block link creation

3. **Invalid Response** (malformed JSON, unexpected structure):
   - Log error
   - Fall back to manual entry
   - Don't block link creation

### Error Messages

- Service unavailable: `"âš ï¸  Scraper service unavailable. Please fill fields manually."`
- Scraping failed: `"âš ï¸  Could not scrape URL: <error>. Continue manually? (press Enter)"`
- Timeout: `"â±ï¸  Scraping timed out. Continue manually? (press Enter)"`

### Graceful Degradation

- If scraper service is not configured, skip scraping entirely
- If scraping fails, allow manual entry
- Never block link creation due to scraping issues

---

## Code Structure

### File Changes Summary

1. **`pkg/config/config.go`** âœ… COMPLETE
   - âœ… `BaseURL` and `ScrapeTimeout` already in config struct
   - âœ… Defaults and config parsing already implemented

2. **`pkg/cli/app.go`** â³ TODO
   - â³ Add scraper service initialization
   - â³ Pass scraper service to add link form
   - â³ Update `addLinkForm` struct and methods
   - â³ Add scraping state management

3. **Already implemented**:
   - âœ… `pkg/scraper/client.go` - Scraper HTTP client with nginx routing support
   - âœ… `pkg/cli/client/` - API HTTP client
   - âœ… `nginx/nginx.conf` - Reverse proxy routing configuration
   - âœ… `docker-compose.yml` - Service orchestration with nginx

---

## Testing Strategy

### Unit Tests

- Test scraper service initialization
- Test scraping error handling
- Test form state transitions

### Integration Tests

1. **Happy Path**:
   - Configure scraper service URL
   - Run CLI add command
   - Verify scraping happens
   - Verify form auto-fills
   - Verify link created in API

2. **Error Paths**:
   - Scraper service unavailable
   - Scraping fails
   - Invalid URL
   - Timeout

3. **Configuration**:
   - Missing scraper service URL (should skip scraping)
   - Invalid scraper service URL
   - Config override via command line

### Manual Testing

- Test with various URL types (articles, blogs, documentation)
- Test with slow-loading URLs
- Test with invalid URLs
- Test scraper service restart/recovery

---

## Implementation Tasks

### Phase 1: Configuration âœ… COMPLETE

- [x] Add `BaseURL` to config struct (replaces `APIBaseURL`)
- [x] Add `ScrapeTimeout` to config struct
- [x] Update `DefaultConfig()` with defaults
- [x] Config loading and saving implemented
- [x] Nginx reverse proxy configured and routing working

### Phase 2: Scraper Service Integration â³ TODO

- [ ] Add scraper service client to `App` struct
- [ ] Implement `getScraperService()` method
- [ ] Add health check on initialization (optional)
- [ ] Test scraper service connection via nginx

### Phase 3: Form Enhancement

- [ ] Update `addLinkForm` struct with scraping fields
- [ ] Add scraping message types (`scrapeStartMsg`, `scrapeSuccessMsg`, `scrapeErrorMsg`)
- [ ] Implement `scrapeURL()` method
- [ ] Update `Update()` method to handle scraping messages
- [ ] Update `View()` method to show scraping state
- [ ] Auto-fill form fields from scrape response

### Phase 4: Error Handling

- [ ] Implement graceful error handling
- [ ] Add user-friendly error messages
- [ ] Implement fallback to manual entry
- [ ] Test error scenarios

### Phase 5: Testing & Polish

- [ ] Test full flow end-to-end
- [ ] Test error scenarios
- [ ] Update documentation
- [ ] Add CLI flag to disable scraping (`--no-scrape`)

---

## Acceptance Criteria

### Functional Requirements

- [x] Scraper service client exists (`pkg/scraper/client.go`)
- [x] CLI can configure base URL (via `base_url` in config)
- [x] Nginx reverse proxy routes requests correctly
- [ ] CLI automatically scrapes URLs when adding links
- [ ] Form auto-fills title and text from scraped data
- [ ] User can edit scraped data before submission
- [ ] CLI handles scraper service errors gracefully
- [ ] Link creation works even if scraping fails
- [ ] CLI can skip scraping (via config or flag)

### Non-Functional Requirements

- [x] Scraping timeout is configurable (default: 30 seconds) - `ScrapeTimeout` in config
- [ ] Scraping doesn't block CLI indefinitely
- [ ] Error messages are user-friendly
- [ ] Loading indicators show scraping progress
- [x] Configuration can be set via config file (`~/.config/link-mgmt/config.toml`)

### User Experience

- [ ] User sees clear indication when scraping is happening
- [ ] User can proceed manually if scraping fails
- [ ] Scraped data is clearly distinguished from user input
- [ ] Form flow is smooth and intuitive

---

## Future Enhancements

### Phase 2 Features (Not in Initial Implementation)

1. **Batch Scraping**: Scrape multiple URLs at once
2. **Scrape Cache**: Cache scraped content to avoid re-scraping
3. **Scrape Preview**: Show preview of scraped content before auto-filling
4. **Custom Scrape Timeouts**: Per-URL timeout configuration
5. **Scrape Retry**: Automatic retry on failure
6. **Background Scraping**: Scrape in background while user edits

### Configuration Enhancements

- Environment variable support: `LINK_MGMT_SCRAPER_SERVICE_URL`
- Scrape timeout per URL type
- Disable scraping globally via config

---

## Dependencies

### Existing Dependencies

- âœ… `link-mgmt-go/pkg/scraper/client.go` - Scraper HTTP client (uses nginx routing)
- âœ… `link-mgmt-go/pkg/cli/client/` - API HTTP client (uses nginx routing)
- âœ… `nginx/nginx.conf` - Reverse proxy configuration
- âœ… `docker-compose.yml` - Service orchestration with nginx
- âœ… Bubble Tea - TUI framework (already in use)

### Infrastructure

- âœ… **Nginx Reverse Proxy**: Routes all requests through port 80
    - `/api/*` â†’ API service (api-dev:8080)
    - `/scrape` â†’ Scraper service (scraper-dev:3000)
    - `/scraper/*` â†’ Scraper service (scraper-dev:3000)
- âœ… **Docker Compose**: Orchestrates all services with proper networking
- âœ… **Health Checks**: Configured for all services

### No New Dependencies Required

All necessary packages and infrastructure are already available in the codebase.

---

## Success Metrics

1. **Functionality**: Links can be created with auto-scraped content
2. **Reliability**: Scraping failures don't block link creation
3. **User Experience**: Smooth flow with clear feedback
4. **Performance**: Scraping completes within reasonable time (< 30s)
5. **Error Handling**: All error scenarios handled gracefully

---

## Related Documents

- [`service-implementation.md`](../scraper/service-implementation.md) - Scraper service implementation
- [`cli-implementation-plan.md`](./cli-implementation-plan.md) - CLI implementation status
- [`go-api-cli-design-document.md`](./go-api-cli-design-document.md) - Overall Go CLI design

---

**Last Updated**: Updated to reflect nginx reverse proxy architecture
**Next Steps**: Implementation Phase 2 (Scraper Service Integration) and Phase 3 (Form Enhancement)

## Architecture Notes

### Nginx Reverse Proxy Setup

The system uses nginx as a reverse proxy to simplify service access:

- **Single Entry Point**: All services accessed via `http://localhost` (port 80)
- **Service Routing**:
    - API: `http://localhost/api/v1/*` â†’ `api-dev:8080`
    - Scraper: `http://localhost/scrape` â†’ `scraper-dev:3000`
    - Scraper (with prefix): `http://localhost/scraper/*` â†’ `scraper-dev:3000/*`
- **Health Checks**:
    - `/health` â†’ API health
    - `/scraper/health` â†’ Scraper health
- **Timeouts**: Extended timeouts for scraping operations (120s) to handle long-running requests

### Docker Compose Services

- **nginx**: Reverse proxy (port 80)
- **api-dev**: Go API service (port 8080, exposed internally)
- **scraper-dev**: Bun scraper service (port 3000, exposed internally)
- **postgres**: Database (port 5432)

All services run in the `dev` profile and communicate via Docker networking.
